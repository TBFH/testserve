# Adapted from github.com/vllm/worker/worker.py
import copy
import time
from typing import List, Tuple, Optional

import torch
import torch.distributed
import ray

from testserve.config import ModelConfig, CacheConfig, ParallelConfig, SchedConfig
from testserve.request import Request, BatchedRequests
from testserve.utils import set_random_seed
from testserve.models import get_model_op
from testserve.utils import get_gpu_memory, set_random_seed, GB, MB
from testserve.logger import init_logger
from testserve.tokenizer import get_tokenizer
from testserve.downloader import download_and_convert_weights


logger = init_logger(__name__)

# def gpu_inspect(rank):
#     outlog = ""
#     # Actor&Node Affiliation
#     node_id = ray.get_runtime_context().get_node_id()
#     outlog += f"[Rank{rank}] Deployed at Node: {node_id}\n"
#     # GPU Overall Inspect
#     import pycuda.driver as cuda
#     cuda.init()
#     device = cuda.Device(0)
#     outlog += f"[Rank{rank}] GPU Device {0}: {device.name()}\n"
#     context = device.make_context()
#     total_memory = device.total_memory() / (1024 ** 3)
#     free_memory = cuda.mem_get_info()[0] / (1024 ** 3)
#     used_memory = total_memory - free_memory
#     outlog += f"[Rank{rank}] Used/Total VRAM: {used_memory:.1f}/{total_memory:.1f} GB ({(used_memory/total_memory)*100:.1f}%)\n"
#     # outlog += f"[Rank{rank}] Used VRAM: {used_memory:.1f} MiB\n"
#     outlog += f"[Rank{rank}] Free VRAM: {free_memory:.1f} GB\n"
#     context.pop()
#     print(outlog)
#     # Program GPU Utilization
#     # total_memory = torch.cuda.get_device_properties(0).total_memory / (1024 ** 2)
#     # allocated_memory = torch.cuda.memory_allocated(0) / (1024 ** 2)
#     # reserved_memory = torch.cuda.memory_reserved(0) / (1024 ** 2)
#     # outlog += f"[Rank{rank}] Allocated Memory: {allocated_memory:.2f} MiB\n"
#     # outlog += f"[Rank{rank}] Reserved Memory: {reserved_memory:.2f} MiB\n"



# If we call `torch.ops.swapping_ops.swap` in `ParaWorker.swap_blocks()` directly,
# it will result in a "cannot pickle" error. Don't know why
def call_swapping_op(
    source_block_ids: List[int],
    target_block_ids: List[int],
    is_swap_in: bool,
    k_cache: torch.Tensor,
    v_cache: torch.Tensor,
    k_swap: torch.Tensor,
    v_swap: torch.Tensor,
):
    """Call the swapping operation."""
    # The swapping operation is a custom C++ operator that swaps the blocks
    # between the CPU and GPU. The operator is defined in
    # FastServe/fastserve/swapping_ops.cpp.
    torch.ops.swapping_ops.swap(
        source_block_ids,
        target_block_ids,
        is_swap_in,
        k_cache,
        v_cache,
        k_swap,
        v_swap,
    )

@ray.remote(num_cpus=0, num_gpus=1)
class ParaWorker:
    """A worker class that executes (a partition of) the model on a GPU.

    Each worker is associated with a single GPU. The worker is responsible for
    maintaining the KV cache, the KV swap and executing the model on the GPU.
    In case of distributed inference, each worker is assigned a partition of
    the model.

    """

    def __init__(
        self,
        model_config: ModelConfig,
        cache_config: CacheConfig,
        sched_config: SchedConfig,
        parallel_config: ParallelConfig = ParallelConfig(),
        tensor_parallel_id: List[int] = None,
        pipeline_parallel_id: List[int] = None,
    ) -> None:
        self.model = None
        self.model_config = model_config
        self.parallel_config = parallel_config
        self.cache_config = cache_config
        self.sched_config = sched_config
        self.tensor_parallel_id = tensor_parallel_id
        self.pipeline_parallel_id = pipeline_parallel_id
        self.tokenizer = get_tokenizer(
            model_config.tokenizer,
            tokenizer_mode=model_config.tokenizer_mode,
            trust_remote_code=model_config.trust_remote_code,
        )
        self.device = torch.device(f"cuda:0")
        torch.cuda.set_device(self.device)
        # K/V cache on GPU
        self.k_cache = None
        self.v_cache = None
        # K/V swap on CPU
        self.k_swap = None
        self.v_swap = None
        # CUDA streams for swapping in and out
        self.swap_in_stream = torch.cuda.Stream()
        self.swap_out_stream = torch.cuda.Stream()
        # The swap_event_table, refer to block_manager.py for more details
        self.swap_event_table = {}
        # The latest swap event in each stream
        # Used when we need to wait for all swap events to finish
        self.latest_swap_in_event = None
        self.latest_swap_out_event = None
        # Statistics
        self.execution_time = 0.0
        self.blocked_swapping_time = 0.0
        # Intermediate results buffer for pipeline_parallel
        self.intermed_input = None
        self.intermed_output = None

    def ready(self):
        """
        Ray functions queue inside one single actor to be executed in order.
        If ready is called, the actor is ready.
        """
        # gpu_inspect(self.parallel_config.pipeline_parallel_rank)
        pass

    def resource_inspect(self):
        node_id = ray.get_runtime_context().get_node_id()
        # GPU Overall Inspect
        import pycuda.driver as cuda
        cuda.init()
        device = cuda.Device(0)
        device_name = device.name()
        context = device.make_context()
        total_memory = device.total_memory() / (1024 ** 2)
        free_memory = cuda.mem_get_info()[0] / (1024 ** 2)
        used_memory = total_memory - free_memory
        context.pop()
        return {
            "GPU_Name": device_name,
            "Total_VRAM": total_memory,
            "Used_VRAM": used_memory,
            "Free_VRAM": free_memory,
            "NodeID": node_id
        }

    def init_model(self):
        # Initialize the model.
        set_random_seed(self.model_config.seed)
        self.model = get_model_op(
            self.model_config, self.parallel_config, self.cache_config
        )
        self.model.init_communicator(self.tensor_parallel_id, self.pipeline_parallel_id)
        torch.cuda.synchronize()
        logger.info("nccl initialized")
        if self.model_config.use_dummy_weights:
            self.model.init_dummy_weights()
        else:
            path = download_and_convert_weights(self.model_config)
            self.model.load_weight(path)
        torch.cuda.synchronize()
        logger.info(f"model {self.model_config.model} loaded")

    def init_kvcache_and_swap(self, num_gpu_blocks, num_cpu_blocks):
        # kv shape is [num_gpu_blocks, num_layers, num_local_heads, block_size, head_dim]
        # profile the GPU to get num_gpu_blocks
        kv_cache_shape = (
            num_gpu_blocks,
            self.model_config.get_num_layers(self.parallel_config),
            self.model_config.get_num_heads(self.parallel_config),
            self.cache_config.block_size,
            self.model_config.get_head_size(),
        )
        self.k_cache = torch.empty(
            kv_cache_shape, dtype=self.model_config.get_torch_dtype(), device="cuda"
        )
        self.v_cache = torch.empty(
            kv_cache_shape, dtype=self.model_config.get_torch_dtype(), device="cuda"
        )
        # kv swap is [num_cpu_blocks, num_layers, num_local_heads, block_size, head_dim]
        # We pin memory here in order to leverage cudaMemcpyAsync when swapping
        kv_swap_shape = (num_cpu_blocks,) + kv_cache_shape[1:]
        self.k_swap = torch.empty(
            kv_swap_shape, dtype=self.model_config.get_torch_dtype(), device="cpu", pin_memory=True
        )
        self.v_swap = torch.empty(
            kv_swap_shape, dtype=self.model_config.get_torch_dtype(), device="cpu", pin_memory=True
        )

    def _get_block_size_in_bytes(
        self,
        block_size: int,
        model_config: ModelConfig,
        parallel_config: ParallelConfig,
    ) -> int:
        # the shape of one slot in k/v cache is [num_layers, num_local_heads, block_size, head_dim]
        num_layers = model_config.get_num_layers(parallel_config)
        num_heads = model_config.get_num_heads(parallel_config)
        head_dim = model_config.get_head_size()

        key_cache_size = num_layers * num_heads * block_size * head_dim
        total = key_cache_size * 2
        dtype_size = model_config.get_dtype_size()
        return total * dtype_size

    @torch.inference_mode()
    def _profile_num_available_blocks(
        self,
        block_size: int,
        gpu_memory_utilization: float,
        cpu_swap_space: int,
    ) -> Tuple[int, int]:
        # Profile the memory usage of the model and get the maximum number of
        # GPU and CPU blocks that can be allocated with the remaining free memory.

        # Profile memory usage with max_batch_size requests and the total
        # number of tokens equal to max_tokens_per_batch.
        total_gpu_memory = get_gpu_memory()
        peak_runtime_memory = (
            total_gpu_memory * 0.01
            + self.model_config.get_model_size_in_bytes(
                parallel_config=self.parallel_config
            )
        )
        logger.info(f"runtime peak memory: {peak_runtime_memory / GB:.3f} GB")
        logger.info(f"total GPU memory: {total_gpu_memory / GB:.3f} GB")
        block_size_in_bytes = self._get_block_size_in_bytes(
            block_size, self.model_config, self.parallel_config
        )
        logger.info(
            f"kv cache size for one token: {block_size_in_bytes / block_size / MB:.3f} MB"
        )
        num_gpu_blocks = int(
            (total_gpu_memory * gpu_memory_utilization - peak_runtime_memory)
            // block_size_in_bytes
        )
        num_cpu_blocks = int(cpu_swap_space // block_size_in_bytes)
        num_gpu_blocks = max(num_gpu_blocks, 0)
        logger.info(f"num_gpu_blocks: {num_gpu_blocks}")
        num_cpu_blocks = max(num_cpu_blocks, 0)
        logger.info(f"num_cpu_blocks: {num_cpu_blocks}")

        # Reset the seed to ensure that the random state is not affected by
        # the model initialization and profiling.
        set_random_seed(self.model_config.seed)
        # return num_gpu_blocks, num_cpu_blocks
        return 100, 100

    def step(
        self,
        request_ids: List[int],
        input_tokens_batched,
        first_token_indexes,
        block_table,
        intermed = None
    ) -> Tuple[Optional[List[int]], int]:
        """Run one step of inference on the batch of requests."""

        start = time.time()
        # Check whether synchronization is necessary
        for request_id in request_ids:
            if request_id in self.swap_event_table:
                # We let the current stream wait for the swap event
                # This is non-blocking (It just stop the current stream instead
                # of chocking the CPU)
                self.swap_event_table[request_id].wait(torch.cuda.current_stream())
                self.swap_event_table.pop(request_id, None)
        self.blocked_swapping_time += time.time() - start

        intermed_shape = (
            sum([len(req) for req in input_tokens_batched]),
            self.model_config.get_hidden_size()
        )
        self.intermed_input = torch.empty(
            0, dtype=self.model_config.get_torch_dtype(), device="cuda"
        )
        self.intermed_output = torch.empty(
            intermed_shape, dtype=self.model_config.get_torch_dtype(), device="cuda"
        )

        if not self.parallel_config.is_first_stage() and len(input_tokens_batched) > 0:
            _, inter_in = intermed
            self.intermed_input = inter_in
            # print(f"pp rank: [{self.parallel_config.pipeline_parallel_rank}] intermed_input is {self.intermed_input}")

        # gpu_inspect(self.parallel_config.pipeline_parallel_rank)

        start = time.time()
        # run forward
        generated_tokens_ids = self.model.forward(
            input_tokens_batched,
            first_token_indexes,
            self.k_cache,
            self.v_cache,
            block_table,
            self.intermed_input,
            self.intermed_output
        )
        self.execution_time += time.time() - start

        # if not self.parallel_config.is_last_stage() and len(input_tokens_batched) > 0:
        #     print(f"pp rank: [{self.parallel_config.pipeline_parallel_rank}] intermed_output is {self.intermed_output}")
        
        return generated_tokens_ids, copy.deepcopy(self.intermed_output)

    def swap_blocks(
        self,
        request_ids: List[int],
        source_block_ids: List[int],
        target_block_ids: List[int],
        is_swap_in: bool,
    ):
        """Swap some blocks between CPU and GPU
        If is_swap_in, then move blocks from CPU to GPU, i.e. CPU block
        #source_block_ids[0] will be copied to GPU block #target_block_ids[0]
        and so on. Similar for is_swap_in = False
        """

        # print(f"Swap {source_block_ids} ({'CPU' if is_swap_in else 'GPU'}) to {target_block_ids} ({'GPU' if is_swap_in else 'CPU'})")
        stream = self.swap_in_stream if is_swap_in else self.swap_out_stream

        # Record event
        event = torch.cuda.Event()
        event.record(stream)

        # Save that event
        for request_id in request_ids:
            if request_id in self.swap_event_table:
                # If we've issued another swapping operation before, we shall wait it
                # Pay attention to the difference between wait() and synchronize()
                self.swap_event_table[request_id].wait(stream)
            self.swap_event_table[request_id] = event
        if is_swap_in:
            self.latest_swap_in_event = event
        else:
            self.latest_swap_out_event = event

        # Swap
        with torch.cuda.stream(stream):
            call_swapping_op(
                source_block_ids,
                target_block_ids,
                is_swap_in,
                self.k_cache,
                self.v_cache,
                self.k_swap,
                self.v_swap,
            )

    def clear_request_resource(self, request_id: int):
        """Clear the resources associated with the request."""
        """This is called by LLMEngine when a request is finished or aborted"""
        # Clear the swap event table
        self.swap_event_table.pop(request_id, None)

    def clear_request_resource_batched(self, requests: List[Request]):
        """Clear the resources associated with the requests."""
        for request in requests:
            self.clear_request_resource(request.request_id)

    def wait_for_all_swap_in(self):
        """Wait for all swap in to finish"""
        if self.latest_swap_in_event is not None:
            self.latest_swap_in_event.synchronize()
            self.latest_swap_in_event = None

    def wait_for_all_swap_out(self):
        """Wait for all swap out to finish"""
        if self.latest_swap_out_event is not None:
            self.latest_swap_out_event.synchronize()
            self.latest_swap_out_event = None
